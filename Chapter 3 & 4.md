# 第二章 可变形体子空间仿真相关技术

可变形体子空间仿真是计算机图形学领域中一个非常重要的研究方向，其在游戏、动画、VR/AR、软体机器人等领域有着广泛应用。子空间仿真的关键是找到一个高质量的子空间映射，这个映射把低维子空间状态映射到高维全空间状态，子空间映射对子空间仿真的质量和效率至关重要。通过子空间方法降维之后，时间积分求解的瓶颈变成了势能梯度的计算，为了加速子空间仿真，多维求积方法在子空间仿真中广泛应用于计算势能的导数。

本章将从子空间构造和多维求积方法这两个方面的工作进行介绍，并且对子空间中 Lipschitz 优化相关工作进行简要介绍。

## 2.1 子空间构造方法

### 2.1.1 传统线性子空间

传统线性子空间方法通过理论分析或数据驱动的方式找到一组表示仿真形变模式的基，其子空间映射是一个基矩阵 $U$ 乘一个子空间坐标 $z$ 的形式：

$$
\boldsymbol{q} = f(\boldsymbol{z}) = U \boldsymbol{z}
$$

全空间状态由这些形变模式的线性组合表示。

1989 年，Pentland 等人提出了通过线性模态分析构造线性子空间的方法，线性模态分析通过弹性能量的 Hessian 构造线性子空间。Hessian 的特征向量代表物体的形变模式，特征值越大表示与这个形变模式相似的形变更容易发生，因此可以选择特征值较大的特征向量构造线性基，丢弃小特征值对应的形变模式。线性模态分析简单有效，对于物体的发生的小形变能较精确表示，但难以表示大形变。James 等人通过模态导数扩充了线性基，以适应更大的变形。Brandt 等人通过线性混合蒙皮的方式构造线性子空间，在模型上选取部分点作为操纵点 (Handle)，以这些点为中心通过径向基函数 (RBF) 确定每个网格顶点的权重，线性基由蒙皮权重组成，他们将这个线性子空间应用于投影动力学，实现了高效的子空间投影动力学仿真。在具有大量实际仿真数据的情况下，也可以通过主成分分析 (PCA) 构造线性子空间。

仿真过程发生的形变是非线性的，线性子空间方法通过线性基矩阵中形变模式的线性组合近似全空间的仿真状态，因此，线性子空间方法需要较大的子空间维度才能更高精度地表示仿真中的非线性形变。然而，更大的子空间维度又会造成更大的求解开销，这与降维的初衷相违背。因此，如何在更小的维度下更精确地表示全空间状态是当前子空间方法所追求的。

### 2.1.2 基于监督学习的神经子空间

随着深度学习的兴起，神经网络被广泛用于子空间仿真方法中，这类非线性子空间能在较小的维度下捕捉到全空间的一些高度非线性形变，基于神经网络的非线性子空间方法是当前的研究热门。2019 年，Fulton 等人首次使用自编码器从全空间仿真的轨迹数据帧中学习非线性子空间映射。如图 2-1 所示，该网络总体框架为一个编码器-解码器结构，编码器负责将全空间状态压缩到一个低维子空间中，解码器将子空间状态还原为全空间状态。该网络通过优化一个重建损失完成训练，训练完成的解码器将作为子空间映射。为了更有效地训练，他们在自编码器的外层增加一层不可学习的 PCA 层，在 PCA 子空间中训练网络，经过 PCA 降维之后，数据的维度已经大幅度降低，因此神经网络的规模很小，只有两个非线性层。

之后，Shen 等人认为需要用深层的自编码器来捕获仿真中的非线性形变。如图 2-2 所示，网络结构也是一个自编码器，但与 Fulton 等人的方法不同的是，他们的自编码器是一个深层的网络。值得注意的是，自编码器的外层是一个过滤层 (Filter)，Filter 的作用是过滤掉 PCA 子空间能表示的部分，PCA 子空间不能表示的部分通过网络学习，网络是在 PCA 的残差空间中训练的。网络的输入是：

$$
(I - U U^T) \boldsymbol{x}
$$

其中 $U$ 为 PCA 基矩阵，$I$ 为单位阵，$\boldsymbol{x}$ 是物体的位置向量。

他们的方法结合了线性子空间和非线性子空间，子空间映射表示为：

$$
\boldsymbol{q} = U \boldsymbol{p} + D(\boldsymbol{q}), \quad \text{s.t.} \quad U^T D(\boldsymbol{q}) = 0
$$

其中 $\boldsymbol{p}$ 是 PCA 子空间状态，$\boldsymbol{q}$ 是神经子空间状态，PCA 子空间和神经子空间表示的形变正交。这种子空间可以认为是通过一个神经网络给 PCA子空间进行非线性形变修正，使得 PCA 子空间可以在较小的维度下更精确地表示非线性形变，在增加表示精度的同时又不会使子空间仿真成本大幅度提高。

子空间仿真的一个缺陷是接触形变没有得到高度精确的表达，因为接触形变的种类很多并且不平滑，子空间映射无法捕获，使仿真形变看起来过于平滑。Romero 等人提出使用神经网络对接触形变进行学习，以克服线性子空间方法中对接触形变表达不足的缺陷。他们分析了线性子空间在仿真过程中的两个误差来源并提出两个修正项：内部修正和外部修正。通过对大量接触形变的仿真数据进行学习，修正网络针对不同的子空间状态输出内部修正和外部修正，增加了子空间对接触形变的表示能力。

### 2.1.3 基于无监督学习的神经子空间

以上介绍的神经子空间方法依赖于大量的仿真数据进行监督学习，收集这样一个数据集是耗时和繁琐的。最近，Sharp 等人提出了基于无监督学习的非线性子空间方法，该方法不依赖于仿真轨迹数据。仿真中出现的状态都集中在势能较低的区域，因为高势能表示网格模型发生了极端变形，这在物理仿真过程中是极少出现。因此，Sharp 等人通过优化基于物理的势能函数拟合一个神经子空间映射。图 2-3 是他们提出的方法示意图，可以看到，该方法的子空间网络仅包含一个 MLP，他们预设子空间的状态分布是一个低维的高斯分布，通过最小化势能函数使 MLP 将子空间状态映射到高维空间中的低势能状态，训练完成的 MLP 作为子空间映射 $f_\theta$。

但是仅最小化势能函数并不能学习到一个有效的映射，因为网络会把所有子空间状态都映射到势能的最低点，也就是网格模型未发生形变的状态。为了避免这个问题，他们又引入一个排斥损失 (Repulsion Loss)，该损失通过等距约束实现：

$$
\| f_\theta(\boldsymbol{z}) - f_\theta(\boldsymbol{z}') \|_M \approx \sigma \| \boldsymbol{z} - \boldsymbol{z}' \|
$$

其中 $\boldsymbol{z}, \boldsymbol{z}'$ 为高斯分布采样中的任意不同点，$M$ 是网格顶点的质量矩阵，$\sigma$ 为调节等距约束的超参数。将上式写成对数形式并取平方得到排斥损失：

$$
\mathcal{L}_{\text{repulsion}} = \left( \log \frac{\| f_\theta(\boldsymbol{z}) - f_\theta(\boldsymbol{z}') \|_M}{\sigma \| \boldsymbol{z} - \boldsymbol{z}' \|} \right)^2
$$

Sharp 等人的方法解决了当前神经子空间方法中数据依赖问题，仅需从高斯分布中采样就可以获取训练数据，通过优化一个可微的势能函数学习非线性子空间映射。然而他们的方法仅能拟合低维的子空间 ($r \leq 10$)，对于更高维的子空间，他们的方法往往表现不好。

Wang 等人把 Sharp 等人提出的无监督神经子空间方法作为线性模态分析的一个非线性形变修正：

$$
\boldsymbol{q} = U \boldsymbol{z} + \boldsymbol{y}_\theta(\boldsymbol{z}), \quad \text{s.t.} \quad U^T \boldsymbol{y}_\theta = 0
$$

其中 $U$ 是由线性模态分析构造的线性基矩阵，$\boldsymbol{y}_\theta$ 是非线性修正。与 Sharp 等人方法一样，这种子空间方法也只适合拟合较低维度的子空间，当前基于无监督学习的子空间方法一个明显不足是在复杂模型上的仿真效果并不好。

## 2.2 多维求积方法

子空间仿真方法通过降低时间积分中优化问题的维度极大加速了可变形体仿真的效率，但是在优化求解时需要计算势能关于子空间状态的一阶导数或二阶导数，这些导数的计算开销仍与全空间的维度相关，这极大限制了子空间软体仿真的效率。多维求积方法是子空间时间积分中广泛使用的导数计算加速方法。

多维求积 (Cubature) 是一维求积 (Quadrature) 的多维版本，Quadrature 是数值分析中计算一维积分的一种数值方法，通过有限个节点 $x_i$ 处的函数值加权求和来逼近积分值：

$$
\int_a^b f(x) dx \approx \sum_{i=1}^n w_i f(x_i)
$$

以高斯积分 (Gaussian Quadrature) 为例，高斯积分的节点 $x_i$ 通过一个正交多项式的根确定，权重 $w_i$ 通过正交多项式的导数公式获得，$n$ 个采样点就可以精确积分 $2n-1$ 次的多项式。然而，在高维积分中并没有高斯积分这样的通用且有效的积分方案，直接将高斯积分推广到高维会造成维度灾难 (Curse of Dimensionality)，计算量随着维度指数增长。

2008 年，An 等人首次在子空间仿真中使用多维求积方法。他们的方法为两个阶段，预计算和运行阶段。预计算阶段中在一个已经构造好的子空间中进行采样获取仿真数据，通过仿真数据计算出积分子集和权重。在仿真运行阶段，通过这个积分子集和权重近似计算子空间梯度，由于只在一个子集上计算近似梯度，因此比在全集上计算精确梯度要快得多。

在模型采用四面体网格进行离散化的条件下，弹性势能是所有四面体弹性能量之和，对于势能关于子空间状态的梯度计算，需要分别计算每个四面体的梯度然后求和。多维求积方法通过选出一个四面体子集，然后给子集中的每个四面体确定一个非负权重，用这个子集的四面体的梯度进行加权求和近似精确的梯度：

$$
\nabla_{\boldsymbol{z}} P = \sum_{e \in \mathcal{E}} \nabla_{\boldsymbol{z}} P_e \approx \sum_{e \in \mathcal{S}} w_e \nabla_{\boldsymbol{z}} P_e
$$

其中 $P$ 是势能函数，$\nabla_{\boldsymbol{z}} P$ 为势能的子空间梯度，$\nabla_{\boldsymbol{z}} P_e, w_e$ 分别表示四面体 $e$ 的子空间梯度和非负权重，$\mathcal{E}$ 表示网格的所有四面体组成的集合，$\mathcal{S}$ 为 $\mathcal{E}$ 的一个子集。

多维求积的关键是确定子集和权重，在选出子集的情况下，通过最小化整个训练数据集中近似误差确定权重，为了避免过拟合，通过非负最小二乘法求解以下最小二乘问题：

$$
A \boldsymbol{w} = \boldsymbol{b}
$$

其中 $\boldsymbol{A} \in \mathbb{R}^{rT \times m}$，$\boldsymbol{A}$ 是由已选择的四面体的梯度组成的矩阵，$m$ 是已经选择的四面体数，$T$ 为训练数据的帧数，$r$ 是子空间维度，$\boldsymbol{A}$ 的每一列是所有数据帧中四面体 $i$ 的梯度组成的列向量，$\boldsymbol{b}$ 是由所有数据帧的精确梯度组成的列向量，$\boldsymbol{w}$ 是非负权重向量。这里优化的是相对误差，因此线性方程组的每一行除了对应帧的精确梯度的模长。

在子集的构造问题上，An 等人提出了 Greedy Cubature 算法，使用贪心策略动态地构造积分子集。首先子集 $\mathcal{S} = \emptyset$，残差 $\boldsymbol{r} = \boldsymbol{b}$，每次选择可以最小化当前残差的四面体加入集合：

$$
e = \arg\max_{e} \frac{\boldsymbol{g}_e^T \boldsymbol{r}}{\| \boldsymbol{g}_e \| \| \boldsymbol{r} \|}
$$

其中 $\boldsymbol{g}_e^T \boldsymbol{r}$ 表示两个向量的点乘，$\boldsymbol{g}_e^T$ 表示四面体 $e$ 的势能梯度的转置。上式表示取四面体的势能梯度与残差向量 $\boldsymbol{r}$ 夹角最小的四面体加入集合 $\mathcal{S}$。更新子集之后通过非负最小二乘法求解权重，更新残差 $\boldsymbol{r}$。重复子集选取与更新残差的过程，动态地构造出一个关键子集。

An 等人的方法是目前子空间仿真中最常用的子空间积分加速方法，能以与全空间自由度无关的成本近似计算梯度，在使用多维求积近似的情况下实现了数倍的加速比。然而，这种子集选择方法通过贪心策略每次选择一个元素并求解一次非负最小二乘法，随着子集元素数量的增加，重复求解 NNLS 的过程是非常耗时的。Von Tycowicz 等人通过随机的方式一次性选择完整的子集，整个子集在后续过程中通过硬阈值追踪算法优化，减少了 NNLS 的重复求解次数，加快了多维求积方法的训练速度。Yang 等人扩充了子集的非负权重，为子空间状态的每一个分量给定一个权重，扩充了权重之后，NNLS 的维度降为原来的 $1/r$，但是需要求解 $r$ 次，由于 NNLS 可以并行求解，加速了 Cubature 训练时间。

然而，上述方法使用的都是固定权重的方案，在进行子空间仿真的过程中，子集和非负权重都保持不变，这限制了多维求积方法在小子集下的近似精度。最近，Shen 等人提出了一个可变权重的多维求解方法，通过图神经网络选择子集并通过网络拟合非负权重，每次选择多个元素，提高了子集选取的效率。

## 2.3 Lipschitz 正则化相关工作

对于一个函数 $f(x): X \to Y$，存在一个常数 $L \geq 0$，使得对任意的 $x_1, x_2 \in X$，都有：

$$
\| f(x_1) - f(x_2) \| \leq L \| x_1 - x_2 \|
$$

则称 $f(x)$ 为 Lipschitz 连续，其中 $L$ 为 Lipschitz 常数，Lipschitz 常数限制了函数的最大变化速率，它是函数平滑性的一种表示。改变上式中的函数的导数阶数，可以得到 Lipschitz Gradient 连续和 Lipschitz Hessian 连续。

神经网络的输入输出映射平滑度与网络的泛化能力和鲁棒性相关，为了提高网络平滑度，可以对输入输出映射的雅可比矩阵的范数进行惩罚，还可以通过修改网络权重矩阵限制网络的 Lipschitz 常数。

Lipschitz 正则化技术也被引入到图形学中，2022 年 Liu 等人通过 Lipschitz 正则化提高了基于神经场 (Neural Field) 的形状表示的平滑度，使形状插值变得更加平滑。并提出一种可以自动优化 Lipschitz 常数的 Lipschitz 多层感知机 (Lipschitz Multilayer Perceptron, LipMLP)。相比于传统方法 (如 Jacobian 矩阵范数惩罚) 仅鼓励训练数据附近的局部平滑，LipMLP 的 Lipschitz 正则化作用于整个输入空间。在子空间仿真中，Chen 等人提出通过正则化子空间映射来提高基于投影的方法的精度。他们引入了一个速度损失项，该项惩罚两个相邻时间步的子空间映射雅可比矩阵的线性近似误差。

当前 Lipschitz 正则化技术主要用于提高网络输入与输出映射的平滑度以增加网络的泛化能力和鲁棒性。更高阶的 Lipschitz 常数表示函数的梯度和 Hessian 的平滑度，而梯度和 Hessian 对函数的求解有着重要影响。因此，高阶的 Lipschitz 正则化技术是非常有研究价值的。

## 2.4 本章小结

本章主要介绍了与本文中最相关的子空间构造和多维求积相关工作。首先介绍了可变形体仿真和子空间仿真的基本方法。2.1 节介绍了传统线性子空间构造方法和基于神经网络的子空间构造方法，传统线性子空间方法需要较大的子空间维度捕获仿真中的非线性形变，而基于神经网络的非线性子空间方法在构造子空间时可能会引入过度非线性导致子空间仿真效率受限。2.2 节介绍了在加速子空间积分中最广泛应用的多维求积方法，当前在子空间仿真中广泛使用的多维求积方法由于使用固定权重的方案，因此在固定子集大小下近似精度受限。2.3 节简要介绍了 Lipschitz 正则化在图形学中的相关工作。

# 第三章 基于Lipschitz优化的神经子空间仿真加速方法

## 3.1 引言

子空间仿真是可变形体仿真的一个重要加速方法，通过降低优化问题的维度极大加速了可变形体仿真效率。子空间仿真方法的关键是构造一个高质量的子空间，这个子空间尽可能表示仿真中可能出现的状态。随着深度学习技术的兴起，研究者开始广泛使用神经网络学习子空间映射，最常用的方法是使用一个自编码器学习全空间状态的低维表示，解码器则作为子空间映射，从低维空间还原全空间状态。然而，当前关于神经子空间方法的研究工作在构造子空间时只专注于在尽可能低维的子空间更精确地表示全空间仿真状态，而忽视了神经网络引入的非线性对原优化问题求解的影响。

对于一个无约束的非线性优化问题，可以通过降低它的维度达到加速求解的目的，例如在使用牛顿法求解的情况下，通过降维方法降低了在求解下降方向时线性方程组的规模，因此加速了优化问题的求解，这也是当前子空间相关工作所关注的。从优化问题本身复杂性角度来看，如果一个非线性优化问题是二次的，那么可以在一次牛顿迭代的情况下就可以收敛到全局最小值；如果一个非线性优化问题非常复杂，那么就需要更多的迭代步数才能收敛到极小值。神经网络的引入使得在子空间构造时对势能函数进行非线性优化成为可能，通过对势能函数的复杂性进行优化，降低求解时的迭代次数，达到加速求解的目的。

本章通过引入 $Lipschitz$ 优化来实现势能函数非线性优化目标，在子空间拟合网络训练时优化势能函数关于子空间状态的二阶 $Lipschitz$ 常数，并使用多维求积近似二阶 $Lipschitz$ 损失函数的计算，加速训练和降低显存消耗。

本章的主要研究内容如下：

(1) 提出一个包含数据集获取的仿真框架。该仿真框架可以生成随机交互序列，通过预设的交互序列实现具有随机交互的仿真，并自动保存仿真轨迹数据，避免了数据集生成过程中进行人工交互这一耗时的过程，也能减少获取的仿真轨迹数据集的偏向性。

(2) 提出了一个基于 $Lipschitz$ 优化的神经子空间仿真加速方法。由于神经网络引入的非线性会影响求解的效率，导致神经子空间仿真效率受限，因此本章在神经网络构造神经子空间时引入 $Lipschitz$ 损失函数，对势能函数在子空间的复杂性进行优化，构造在仿真中能高效求解的神经子空间，并且通过多维求积方法近似计算 $Lipschitz$ 损失函数，加速训练和降低显存消耗。

(3) 将提出的神经子空间可变形体仿真加速方法扩展到基于无监督学习的神经子空间方法。本章提出的 $Lipschitz$ 优化依赖于多维求积方法，而无监督神经子空间方法并不能通过数据集预先确定多维求积子集和权重，因此不能直接使用多维求积方法加速训练。本章通过训练一个权重预测网络的方式将 $Lipschitz$ 优化应用于基于无监督学习的神经子空间方法上。

(4) 对提出的方法的有效性进行实验验证及分析。为了评估本章提出的加速方法的有效性，将与未进行 $Lipschitz$ 优化的神经子空间方法进行仿真速度对比和精度对比，证明本章提出的方法在加速神经子空间仿真上的有效性。同时，对子空间维度大小、多维求积方法子集大小对仿真及训练的影响进行讨论。

## 3.2 仿真框架搭建及数据集获取

基于监督学习的神经子空间仿真方法需要全空间状态帧作为训练数据，通常有两种获取方法。一是在一个已经构造好的线性子空间中采样，比如从高斯分布中采样一些点，经过线性基映射到全空间，得到训练数据。这种方法获取训练数据速度快，但是获取的状态帧未必能覆盖仿真中最可能出现的状态或出现一些奇异状态。二是从全空间仿真中获取轨迹数据：运行全空间仿真，保存仿真中的每一帧状态作为训练数据，这种从仿真中获取数据的方法可以获得高质量的训练数据。但是由于要进行全空间的仿真，并且为了获取覆盖范围更广泛的数据集需要进行带有交互的仿真，因此获取数据的效率较低。为了得到高质量的数据集，提升子空间仿真的质量，本文采用第二种方式构造数据集，这也是当前子空间仿真研究工作中最常用的一种。

### 3.2.1 仿真框架搭建

为了高效地获取数据集并减少数据集构建时的人工操作，本文搭建了一个可以进行随机交互的仿真框架。如图 3-1 所示，仿真器接受一个初始场景和一个随机交互序列输入，仿真器初始化完成之后会根据模型当前状态 $\boldsymbol{q}_t$ 的受力情况及时间步长进行隐式时间积分，通过求解器求解隐式时间积分，得到下一个时刻的状态 $\boldsymbol{q}_{t+1}$；完成一次时间步进之后，$\boldsymbol{q}_{t+1}$ 则变为当前状态继续求解下一时刻的状态，在图 3-1 中对应 $t = t+1$ 这一过程。

为了在大时间步下实现稳定地仿真，本文采用隐式时间积分作为积分方案，根据牛顿第二定律的速度、力、与位置的物理关系：

$$
\begin{cases}
\boldsymbol{q}^{t+1} = \boldsymbol{q}^t + h \boldsymbol{v}^{t+1} \\
\boldsymbol{v}^{t+1} = \boldsymbol{v}^t + h M^{-1} \boldsymbol{F}(\boldsymbol{q}^{t+1})
\end{cases}
$$

其中 $\boldsymbol{q} \in \mathbb{R}^n$ 是当前网格顶点位置组成的列向量，$n$ 为网格的自由度数，$h$ 是时间步长，本文设为 $0.05s$，$\boldsymbol{v} \in \mathbb{R}^n$ 为速度向量，$M \in \mathbb{R}^{n \times n}$ 为网格所有顶点质量组成的质量矩阵，$\boldsymbol{F}(\cdot)$ 表示网格顶点受到的力，它与 $\boldsymbol{q}$ 相关，上标 $t$ 表示当前时刻，$t+1$ 表示下一时刻。式 (3-1) 的物理意义是：下一时刻的位置由当前时刻的位置加上速度乘以时间间隔计算，下一时刻的速度等于当前时刻的速度加上加速度乘以时间间隔。

式 (3-1) 中 $\boldsymbol{F}(\boldsymbol{q})$ 是非线性的，可以将它用一阶泰勒展开线性化后求解线性方程组，本文采用求解精度更高的优化问题的形式：

$$
\boldsymbol{q} = \arg\min_{\boldsymbol{q}} \left[ \frac{1}{2h^2} \| \boldsymbol{q} - \overline{\boldsymbol{q}} \|_M^2 + P(\boldsymbol{q}) \right]
$$

其中 $\| \cdot \|_M^2$ 表示矩阵范数，$\| \boldsymbol{x} \|_M^2 = \boldsymbol{x}^T M \boldsymbol{x}$，$\overline{\boldsymbol{q}} = \boldsymbol{q}^t + h \boldsymbol{v}^t$ 是根据当前状态的一个猜测位置，$P$ 表示弹性势能，这里使用 Stable Neo-Hookean 能量，该能量具有四面体翻转下稳定的特性，在大变形的仿真下具有极强的稳定性。使用 L-BFGS 算法对式 (3-2) 进行求解。

仿真框架中交互的处理方式是使用交互弹簧。具体地，给定一个抓取点，在抓取位置添加一根弹簧，弹簧的一端为抓取点，另一端为目标点，在弹力的作用下，物体发生形变。采用这种交互方式的一个好处是交互弹簧能量 $S(\boldsymbol{q})$ 和弹性势能 $P(\boldsymbol{q})$ 都是关于位置的势能，在求解时两者可以统一为势能处理，而不用改变原有框架。如图 3-2 所示，交互序列包含用户预先生成的多个交互，每个交互包括以下信息：交互开始帧、交互结束帧、抓取顶点及其权重和弹簧方向，这里有顶点权重是因为多个顶点分担同一根弹簧的弹力，权重由顶点到弹簧起始点的距离确定。通过这些信息可以确定每个交互发生的时间、持续时间、交互发生的位置和拖拽方向。

可以通过人工输入的方式生成自己想要的交互序列，但数据集的生成需要进行大量的交互，手动输入的方式生成交互序列显然不现实也不合理。本文通过随机的方式生成交互序列中的交互，为了在较少的抓取点的情况下生成尽可能分散并且覆盖整个模型的交互点，采用最远点采样 (Farthest Point Sampling, FPS) 获取抓取中心，在抓取中心一定半径范围内获取抓取顶点，之后采用均匀随机的方式生成弹簧方向，具体操作如下：

(1) 预处理四面体网格顶点，去除网格中受约束的固定点和内部顶点，只采样四面体网格中未受约束的表面顶点；

(2) 随机选择一个顶点，作为第一个交互的抓取中心，加入交互点集合，更新所有顶点到交互点集合的距离信息；

(3) 选择与交互点集合距离最远的一个顶点加入交互点集合，更新所有顶点到交互点集合的距离信息；

(4) 重复步骤 (3)，直至选出预设数量的顶点；

(5) 遍历交互点集合，根据每个抓取中心点和抓取半径确定每个交互的抓取顶点，这些抓取顶点根据到中心顶点的距离通过径向基函数确定权重，交互弹簧的方向在以该抓取中心点为球心的单位球内均匀采样，根据预设的交互持续帧、交互间隔帧生成每一个交互。

### 3.2.2 数据集获取

使用图 3-1 的仿真框架进行全空间仿真，整个数据集的构建流程如图 3-3 所示，在初始网格中 FPS 采样获取交互点并通过设置的参数生成随机交互序列，仿真器根据随机交互序列进行具有随机交互的仿真，每次求解完成就会自动保存当前帧。这里的参数包括采样点数、交互持续帧和交互间隔帧。数据集获取流程只需在开始时设置参数，整个过程自动完成，避免了繁琐的人工交互和生成具有偏向性的数据集。

以 Dinosaur 模型为例，本文设置采样点数量为 $140$，交互持续帧为 $20$，交互间隔帧为 $20$，最后无交互帧为 $200$，每两帧保存一次，则 Dinosaur 模型的数据集总帧数为：

$$
\frac{140 \times (20 + 20) + 200}{2} = 2900 \quad \text{帧}
$$

图 3-4 为 Dinosaur 模型数据集可视化的结果，$t_i$ 表示第 $i$ 帧，其中 $t_0$ 表示第 $0$ 帧，也就是模型未发生形变时的状态。数据集中的每一帧仅包含网格模型位置信息，将所有顶点位置保存成一个位置矩阵。

## 3.3 神经子空间仿真加速方法

本章的目标是实现高效的神经子空间仿真，现有的神经子空间方法在非常低维的子空间下就能捕获仿真中的非线性形变，这已经极大加速了可变形体仿真。然而，从优化问题复杂性的角度看，这些神经子空间方法并没有考虑到神经网络过度非线性的引入对求解造成的影响，导致神经子空间仿真速率受限。本章通过引入一个 $Lipschitz$ 损失在子空间构造时优化势能函数的复杂性，构造求解高效的神经子空间，从而达到加速子空间仿真的目标。

### 3.3.1 二阶Lipschitz优化

神经子空间非线性优化的目标是降低子空间仿真时求解时间积分优化问题的复杂性，为了实现这一点，需要找到一个优化问题复杂性的合理表征，描述优化问题的求解难度。

首先考虑一个子空间中隐式时间积分的优化问题：

$$
z^{k+1} = \arg\min_z E(z) = \arg\min_z \left[ \frac{1}{2 \Delta t^2} \| f(z) - \overline{\boldsymbol{q}}^{k+1} \|_M^2 + P(f(z)) \right]
$$

其中 $f: \mathbb{R}^r \to \mathbb{R}^n$ 为低维子空间到高维全空间的映射，$\overline{\boldsymbol{q}}^{k+1}$ 是根据当前状态和外力得到的初始猜测位置，$M \in \mathbb{R}^{n \times n}$ 是质量矩阵，是由每个顶点的质量组成的对角矩阵，$P(\cdot)$ 为弹性势能。

在使用牛顿法等迭代方法求解式 (3-3) 时，求解开销主要由三部分决定：计算目标函数及其导数的开销 $C_{\text{eval}}$，计算下降方向和步长的开销 $C_{\text{dir}}$（求解线性系统计算下降方向和线搜索确定步长），以及迭代次数 $n_{\text{iter}}$，总的求解开销大致为：

$$
n_{\text{iter}} (C_{\text{eval}} + C_{\text{dir}})
$$

通过子空间方法降维，在计算下降方向时求解的线性系统的维度大幅度降低，因此在子空间仿真中，$C_{\text{dir}}$ 的开销大幅度下降。另一方面，求解器的收敛速度（迭代次数）很大程度由优化问题的复杂度决定。使用牛顿法求解时，根据牛顿法的二阶收敛速率：

$$
\| z^{k+1} - z^* \| \leq \mathrm{Lip}[\nabla_z^2 E] \| \nabla_z^2 E(z^*)^{-1} \| \| z^k - z^* \|^2
$$

其中 $z^{k+1}$ 和 $z^k$ 是第 $k+1$ 和 $k$ 次牛顿迭代的结果，$z^*$ 是优化问题 $\arg\min_z E(z)$ 的解，$\mathrm{Lip}[\nabla_z^2 E]$ 是目标函数的二阶 $Lipschitz$ 常数，$\nabla_z^2 E$ 表示目标函数的二阶导数即 Hessian 矩阵。从上式可以看出，求解误差到达某一误差阈值所需的迭代次数与 Hessian 矩阵的 $Lipschitz$ 常数相关。通过以上分析可知，二阶 $Lipschitz$ 常数在优化问题求解过程中影响着每次迭代误差的上界，二阶 $Lipschitz$ 常数越小，误差下降越快。一种极端情况是目标函数是二次的形式，它的二阶导数为常数，则由上式得：

$$
\| z^1 - z^* \| \leq 0 \cdot \| z^0 - z^* \|^2 = 0
$$

从上式可知，一次牛顿迭代就可以求解。因此，本文认为目标函数的二阶 $Lipschitz$ 常数是描述优化问题复杂性的一个合理表示。

找到一个合理的表示之后，本章的高效子空间仿真方法就是在子空间构造中通过 $Lipschitz$ 优化降低势能函数的复杂性，减少求解的时间开销，从而提高仿真的效率。具体地，对于一个给定的子空间映射 $f_{\theta_{\text{init}}}$，通过优化二阶 $Lipschitz$ 常数找到另一个具有较小 $\mathrm{Lip}[\nabla_z^2 E]$ 的子空间映射 $f_{\theta^*}$，同时两者具有相似的表示能力。本章通过定义一个近似二阶 $Lipschitz$ 常数的损失函数来实现 $Lipschitz$ 优化。

同样考虑式 (3-3) 的优化问题，二阶 $Lipschitz$ 常数表示为：

$$
\mathrm{Lip}[\nabla_z^2 E] = \max_{z_i, z_j \in \Omega} \frac{\| \nabla_z^2 E(z_i) - \nabla_z^2 E(z_j) \|}{\| z_i - z_j \|}
$$

其中 $\Omega$ 是子空间映射 $f$ 的定义域，$z_i, z_j$ 为定义域中的任意两点。直接优化上式是不可行的，因为计算精确的 $Lipschitz$ 常数需要遍历定义域中的所有点对，子空间中具有无穷的点，这是不可计算的。为了使 $\mathrm{Lip}[\nabla_z^2 E]$ 可计算，可以使用一个观测值集合来近似它：

$$
\mathrm{Lip}[\nabla_z^2 E] \approx \max_{z_i, z_j \in \mathcal{Z}, z_i \neq z_j} \frac{\| \nabla_z^2 E(z_i) - \nabla_z^2 E(z_j) \|}{\| z_i - z_j \|}
$$

其中 $\mathcal{Z} = \{ z_i \sim \Pi_\theta(z) \}$ 是子空间状态 $z$ 的观测值集合，它们服从某一分布 $\Pi_\theta(z)$。通过上式可以轻易计算 $\mathrm{Lip}[\nabla_z^2 E]$，然而在训练过程中它产生的梯度是空间稀疏的。具体来说，由于 $\mathrm{Lip}[\nabla_z^2 E]$ 的计算最终只涉及一对点对，每一次迭代过程仅优化一对点对，这有可能增大其他点对 $Lipschitz$ 常数。为了解决这个问题，采用 $p$ 范数软化 $\max$ 操作，上式可以写为：

$$
\mathrm{Lip}[\nabla_z^2 E] \approx \left( \sum_{z_i, z_j \in \mathcal{Z}, z_i \neq z_j} \frac{\| \nabla_z^2 E(z_i) - \nabla_z^2 E(z_j) \|^p}{\| z_i - z_j \|^p} \right)^{\frac{1}{p}}
$$

当 $p \to \infty$ 时，$p$ 范数软化还原为原来的 $\max$ 操作，上式与原式等价。

本章取 $p = 2$，丢弃与采样无关的指，并取期望使上式与采样数无关，这样得到本章的 $Lipschitz$ 损失函数：

$$
\mathcal{L}_{\text{LS}} = \mathbb{E}_{z_i, z_j \sim \Pi_\theta(z)} \left[ \frac{\| \nabla_z^2 E(z_i) - \nabla_z^2 E(z_j) \|^2}{\| z_i - z_j \|^2} \right]
$$

上式可以在一个 batch 内计算，并且在优化时产生空间稠密的梯度，一次性优化所有点对。同时，可以通过改变上式中导数的阶数从而产生不同阶数的 $Lipschitz$ 损失函数。

### 3.3.2 子空间拟合网络

本章的工作并不是提出一个新的子空间构造方法，而是提出一个在神经子空间构造中通过 $Lipschitz$ 优化改善子空间构造從而加速神经子空间仿真的方法。因此，本章在 Fulton 等人提出的神经子空间拟合网络的基础上，加入优化势能函数复杂性的目标函数，降低神经子空间仿真在时间积分求解的难度，达到加速仿真的目标。

图 3-5 展示了本章神经子空间构造所采用的网络模型，它由两个部分组成，PCA 层、编码器组成的编码模块和逆 PCA 层、解码器组成的解码模块。首先，全空间仿真轨迹数据帧 $t$ 通过向量化得到位置向量 $\boldsymbol{q}$，经过 PCA 层降维得到其低维表示 $\boldsymbol{p}$，再經過編碼器將 PCA 子空間狀態 $\boldsymbol{p}$ 進一步降維，得到神經子空間表示 $\boldsymbol{z}$，神經子空間表示 $\boldsymbol{z}$ 經過解碼器、逆 PCA 層還原全空間位置向量 $\boldsymbol{q}'$，訓練完成的解碼模塊作為子空間映射 $f_\theta$。整个过程可表示为：

- 编码过程：$\boldsymbol{z} = \text{Encoder}(U^T \boldsymbol{q}) = g_\theta(\boldsymbol{q})$
- 解码过程：$\boldsymbol{q}' = \text{Decoder}(U \boldsymbol{z}) = f_\theta(\boldsymbol{z})$

其中 $U$ 为 PCA 基组成的矩阵。

编码模块由 PCA 层和浅层的多层感知机 (MLP) 组成。为了更有效地训练，通过 PCA 层给数据进行降维，PCA 子空间的大小由降维后再还原的最大顶点误差决定，本章设置最大顶点误差阈值为 $0.001$，在本章的模型中得到的 PCA 子空间大小通常小于 $300$。多层感知机的隐藏层层数设置为 $2$，宽度设置为 $200$。激活函数的选择对于训练和仿真性能具有重要影响，ReLU 通常是最简单有效的激活函数，但是使用 ReLU 训练的模型在仿真中可能会出现伪影。ELU 是 ReLU 的一个可微近似函数，它处处可微，因此本文采用 ELU 作为激活函数。解码器模块的结构和编码器模块对称，多层感知机的隐藏层层数设置为 $2$，宽度为 $200$，激活函数同样采用 ELU。

两个模块的神经网络规模都很小，这是因为经过 PCA 降维后数据的维度大幅度降低，使用规模很小的网络就能学习到更紧凑並且表示精度高的子空间，并且更小的网络在仿真中每个时间步的前向传播和求导的过程时间开销更小。

### 3.3.3 多维求积近似

本章提出的 $Lipschitz$ 损失函数中需要计算势能关于子空间状态 $\boldsymbol{z}$ 的二阶导数，在训练过程中极大增加了计算开销，在使用自动微分计算导数的过程中会产生庞大的计算图，计算图中保存着大量中间变量，因此也带来了巨大的显存开销。具体地，一个参数规模为 $N$ 的网络，batch 的大小为 $B$，网格的单元数量为 $E$，子空间状态 $\boldsymbol{z}$ 的维度为 $r$，全空间状态 $\boldsymbol{q}$ 的维度为 $n$，计算一阶导数需要保存从输入 $\boldsymbol{z}$ 经过网络输出 $\boldsymbol{q}$，再根据 $\boldsymbol{q}$ 计算势能这一过程的中间变量，则计算一阶导数过程中的显存消耗为 $\mathcal{O}(B(r + N + n + E))$。每次反向传播计算二阶导数的一行或一列，则训练过程的显存消耗为：$\mathcal{O}(r B (r + N + n + E))$。通常，子空间维度 $r$ 小于 $100$，而 $n$ 和 $E$ 常可达上万甚至几十万，相比于未加入 $Lipschitz$ 损失的传统方法的消耗 $\mathcal{O}(B(r + N + n))$，$\mathcal{L}_{\text{LS}}$ 的引入造成了巨大的计算和显存开销，尤其是在高分辨率网格的情况下。

为了减少显存消耗和计算量，本章在训练过程中引入了多维求积近似 (Cubature Approximation)。多维求积是在子空间仿真中快速计算梯度的数值积分方法，通过在全集中选出一个子集计算积分，并给定一个非负权重提高近似精度，在当前子空间仿真中广泛應用。假设 $P(\cdot)$ 是网格的弹性势能，在使用多维求积近似的情况下子空间梯度计算为：

$$
\nabla_{\boldsymbol{z}} P = \sum_{e \in \mathcal{E}} \frac{\partial P_e}{\partial \boldsymbol{q}} \frac{\partial f}{\partial \boldsymbol{z}} \approx \sum_{e \in \mathcal{S}} w_e \frac{\partial P_e}{\partial \boldsymbol{q}} \frac{\partial f}{\partial \boldsymbol{z}}
$$

其中 $\frac{\partial P_e}{\partial \boldsymbol{q}} \frac{\partial f}{\partial \boldsymbol{z}}$，$w_e$ 分别表示四面体 $e$ 的梯度和非负权重，$\mathcal{E}$ 表示网格的所有四面体组成的集合，$\mathcal{S}$ 为 $\mathcal{E}$ 的一个子集。

多维求积方法的关键在于选取子集和确定权重，在这求解积分子集和权重这个问题已经有很多研究工作 [21][22][26][27]，在第二章相关工作部分有详细介绍，考虑到构造子集的效率，在本章中使用 Von Tycowicz 等人 [22] 提出的方法。

多维求积的子集选取和权重确定可以看成一个稀疏近似问题：

$$
\min_{\boldsymbol{w}} \| A \boldsymbol{w} - \boldsymbol{b} \|^2 \quad \text{s.t.} \quad \boldsymbol{w} \geq 0, \quad \| \boldsymbol{w} \|_0 \leq S
$$

其中 $\boldsymbol{w}$ 为权重组成的向量，$\| \boldsymbol{w} \|_0$ 是 $\boldsymbol{w}$ 中非零元素的个数，$A \in \mathbb{R}^{rT \times m}$ 是由训练数据帧和被选择四面体的势能梯度组成的矩阵，$\boldsymbol{b} \in \mathbb{R}^{rT}$ 是训练数据帧的精确梯度组成的向量，分别定义为：

$$
A = \begin{bmatrix}
\frac{\boldsymbol{g}_1^1}{\| \boldsymbol{g}^1 \|} & \cdots & \frac{\boldsymbol{g}_m^1}{\| \boldsymbol{g}^1 \|} \\
\vdots & \ddots & \vdots \\
\frac{\boldsymbol{g}_1^T}{\| \boldsymbol{g}^T \|} & \cdots & \frac{\boldsymbol{g}_m^T}{\| \boldsymbol{g}^T \|}
\end{bmatrix}, \quad
\boldsymbol{b} = \begin{bmatrix}
\frac{\boldsymbol{g}^1}{\| \boldsymbol{g}^1 \|} \\
\vdots \\
\frac{\boldsymbol{g}^T}{\| \boldsymbol{g}^T \|}
\end{bmatrix}
$$

其中 $T$ 表示训练数据帧数，$m$ 表示当前已经选择的单元数，$\boldsymbol{g}_i^t$ 表示第 $i$ 个被选择单元在第 $t$ 帧的梯度，$\boldsymbol{g}^t$ 表示第 $t$ 帧的精确梯度。

稀疏近似问题在各个学科中都很普遍，已进行大量研究以有效解决它们，硬阈值追踪算法 (Hard Threshold Pursuit, HTP) 是其中有效的一种。将 HTP 算法应用于求解上述稀疏近似问题，并且引入非负约束，算法流程如下：

(1) 选择一个初始权重 $\boldsymbol{w}_0$，保留 $\boldsymbol{w}_0$ 中前 $S$ 大的非负分量，其他分量置零，此时根据 $\boldsymbol{w}_0$ 中非零元素已经确定了一个子集 $\mathcal{S}_0$；

(2) 使用梯度下降求解 $\min_{\boldsymbol{w}} \| A \boldsymbol{w} - \boldsymbol{b} \|^2$；

(3) 对权重 $\boldsymbol{w}^{i+1}$ 进行投影操作 $\mathcal{H}_S^+$，保留 $\boldsymbol{w}$ 中前 $S$ 大的非负分量，其他分量置零，根据权重 $\boldsymbol{w}^{i+1}$ 中非零元素更新子集 $\mathcal{S}_i$；

(4) 求解非负最小二乘法再次更新权重 $\boldsymbol{w}^{i+1} = \text{NNLS}(\| A \boldsymbol{w}^{i+1} - \boldsymbol{b} \|^2)$；

(5) 重复 (2)(3)(4) 直至 (3) 中更新子集步骤中子集元素不再发生变化。

一次 HTP 迭代表示为：

$$
\begin{aligned}
\boldsymbol{w}^{i+1} &= \mathcal{H}_S^+(\boldsymbol{w}^i - \mu^i A^T (A \boldsymbol{w}^i - \boldsymbol{b})) \\
\boldsymbol{w}^{i+1} &= \text{NNLS}(\| A \boldsymbol{w}^{i+1} - \boldsymbol{b} \|^2)
\end{aligned}
$$

上述算法称为非负约束的硬阈值追踪算法 (Non-negativity-constrained Hard Threshold Pursuit, NN-HTP)。在使用 NN-HTP 算法确定多维求积中的子集和权重时，初始子集是随机选取的，本章选择 $S$ 大小的子集，设定最大迭代次数为 $20$。

图 3-5 展示了不同子集大小下多维求积近似的结果，Bunny 模型有 $53K$ 个四面体。可以看到，随着子集大小的增加梯度近似相对误差降低，并且在子集大小小于 $1000$ 时就有 $10\%$ 以下的误差，这极大降低了梯度计算的时间消耗。其中，相对误差的计算方式为：

$$
\epsilon = \sqrt{ \frac{1}{T} \sum_{i=1}^T \frac{\| \tilde{\boldsymbol{g}}_i - \boldsymbol{g}_i \|^2}{\| \boldsymbol{g}_i \|^2} }
$$

这里 $T$ 为测试数据的帧数，$\boldsymbol{g}_i$ 为第 $i$ 帧的精确梯度，$\tilde{\boldsymbol{g}}_i$ 为第 $i$ 帧的近似梯度。

值得注意的是，本章的多维求积近似在训练时计算势能函数的二阶导数使用，而在确定多维求积方法的子集和权重时使用的度量指标是梯度，下面将通过实验说明多维求积方法在近似二阶导数的有效性。图 3-6 展示了梯度、Hessian、Lipschitz 损失函数随子集大小的相对近似误差变化，这里使用的子集和权重是使用梯度度量下确定的。从图中可以看出，在子集较小时，三者的近似误差差别较大；随着子集的增大三者逐渐接近，并且一阶导数和二阶导数的近似误差趋于一致。从整体上看，三者的近似误差都是随着子集的增大而下降，并且趋势一致，从数值上看，三者的近似误差在子集大小 $500$ 左右就达到了较低值。因此，可以说明用梯度度量指标确定的子集和权重在近似二阶导数上也是有效的。

通过多维求积近似二阶导数极大降低了子空间网络训练时的计算量和空间消耗，训练时的显存开销由 $\mathcal{O}(r B (r + N + n + E))$ 降为 $\mathcal{O}(r B (r + N + S))$，$S$ 通常小于 $600$，而 $n$ 和 $E$ 通常为几万甚至数十万。

### 3.3.4 损失函数

本章提出的子空间仿真加速方法使用一个自编码器拟合非线性子空间，并且在构造子空间时进行二阶 $Lipschitz$ 优化降低势能函數在子空間的複雜性，在仿真時達到加速求解的目的。因此子空間擬合網絡采用的損失函數包括重建損失和 $Lipschitz$ 損失：

$$
\mathcal{L} = \mathcal{L}_C + \lambda \mathcal{L}_{\text{LS}}
$$

其中 $\lambda$ 为超参数，控制仿真质量和求解收敛速率之间的权重，对于较大的 $\lambda$，网络倾向于学习 $Lipschitz$ 常数小的子空间降低重建的精度，从而影响仿真的质量。$\mathcal{L}_C$ 为重建损失，在一个大小为 $N$ 的 batch 中：

$$
\mathcal{L}_C = \frac{1}{N} \sum_{i=1}^N \| \boldsymbol{q}_i - f_\theta(g_\theta(\boldsymbol{q}_i)) \|^2
$$

$\mathcal{L}_{\text{LS}}$ 为第 3.3.1 节中推导的 $Lipschitz$ 损失：

$$
\mathcal{L}_{\text{LS}} = \frac{1}{N(N-1)} \sum_{i=1}^N \sum_{j=1}^N \frac{\| \nabla_z^2 E(z_i) - \nabla_z^2 E(z_j) \|^2}{\| z_i - z_j \|^2}, \quad i \neq j
$$

## 3.4 基于无监督学习的神经子空间

上述神经子空间构造方法是数据驱动的，通过大量数据进行监督学习，使用基于几何的损失函数拟合非线性子空间，获取这样一个高质量的数据集是非常耗时的。最近，Sharp 等人提出了一个无需仿真轨迹数据集的基于无监督学习的神经子空间方法，他们使用基于物理的损失函数拟合非线性子空间。如图 3-7 所示，该子空间拟合网络仅包含一个 MLP，从高斯分布中采样获取子空间状态 $\boldsymbol{z}$，经过 MLP 映射到全空间状态 $\boldsymbol{q}$，这个 MLP 作为子空间映射 $f_\theta$。由于仿真中模型出现的状态都集中在低势能状态，因此可以利用势能函数来指导神经网络的学习，而无需使用基于仿真的训练数据。通过最小化势能，使神经网络将高斯分布的采样 $\boldsymbol{z}$ 映射到低势能的状态，所用的损失函数是：

$$
\mathcal{L}_p = \mathbb{E}_{\boldsymbol{z} \sim \mathcal{N}} [ E_{\text{pot}}(f_\theta(\boldsymbol{z})) ]
$$

式中 $\mathbb{E}$ 表示随机变量 $\boldsymbol{z}$ 的期望，$\mathcal{N}$ 表示 $r$ 维的高斯分布，$E_{\text{pot}}(\cdot)$ 为势能函数。

然而，最小化上式会产生一个单点映射：所有的高斯分布采样 $\boldsymbol{z}$ 都会映射到势能的最低状态，即网格未发生形变的状态。为了避免这个问题，他们给网络加入一个等距约束：

$$
\| f_\theta(\boldsymbol{z}) - f_\theta(\boldsymbol{z}') \|_M \approx \sigma \| \boldsymbol{z} - \boldsymbol{z}' \|
$$

其中 $\boldsymbol{z}, \boldsymbol{z}'$ 为高斯分布采样中的任意不同点，$M$ 是网格顶点的质量矩阵，$\sigma$ 为调节等距约束的超参数。上式可以写成对数的形式：

$$
\log \frac{\| f_\theta(\boldsymbol{z}) - f_\theta(\boldsymbol{z}') \|_M}{\sigma \| \boldsymbol{z} - \boldsymbol{z}' \|}
$$

对上式取平方得到等距约束损失：

$$
\mathcal{L}_{\text{iso}} = \left( \log \frac{\| f_\theta(\boldsymbol{z}) - f_\theta(\boldsymbol{z}') \|_M}{\sigma \| \boldsymbol{z} - \boldsymbol{z}' \|} \right)^2
$$

则整个网络的损失函数为：

$$
\mathcal{L} = \mathcal{L}_p + \mu \mathcal{L}_{\text{iso}}
$$

Sharp 等人提出的无监督神经子空间方法克服了之前的神经子空间方法的数据依赖问题，只需要一个可微分的势能函数，神经网络就可以从高斯分布中采样获取训练数据拟合一个神经子空间，避免了获取数据集这一耗时的过程。在本节中，将本章提出的基于 $Lipschitz$ 优化的神经子空间加速方法扩展到基于无监督学习的子空间方法中。

### 3.4.1 子空间Lipschitz优化

在 3.3 节中，介绍了基于 $Lipschitz$ 优化的神经子空间加速方法，在神经网络拟合子空间时加入 $Lipschitz$ 损失函数构造求解高效的神经子空间，加速子空间仿真。由于二阶 $Lipschitz$ 损失需要计算势能关于子空间状态 $\boldsymbol{z}$ 的二阶导数，在网络训练时计算量和显存消耗都难以接受，因此引入多维求积近似计算二阶导数，大幅度降低了训练时的计算和显存开销，使得训练得以正常进行。

在无监督方法中加入 $Lipschitz$ 优化目标函数同样也会面临计算和显存开销大的问题。按照前文所述，可以使用多维求积近似计算势能的二阶导数，但是多维求积方法需要训练数据和一个已经确定的子空间，因为在选择子集和计算权重的过程是在子空间下进行的。3.3 节的神经子空间网络的外层是参数冻结的 PCA 层，网络是在 PCA 子空间下学习一个更紧凑的子空间，因此可以在 PCA 子空间中使用多维求积近似势能关于 PCA 子空间状态的二阶导数。而在无监督神经子空间方法中并没有这样固定参数的线性层，在学习的过程中子空间映射在不断变化，所以下面将介绍一种新的方法，在子空间拟合网络训练的过程中同时训练一个多维求积权重预测网络，使用权重网络输出的权重来计算势能的二阶导数。

如图 3-8 所示，在 Sharp 等人方法的基础上加入了一个权重网络 $w_\theta$，子空间状态 $\boldsymbol{z}$ 经过子空间网络 $f_\theta$ 映射到全空间状态 $\boldsymbol{q}$，同时也会经过权重网络 $w_\theta$ 输出当前已选择子集的权重，这里 $m$ 为子集大小，为了输出非负的权重，在权重网络的输出层之后进行平方。输出的权重用于近似计算势能的二阶导数，解决在 $Lipschitz$ 优化目标下计算和显存开销问题。这里子空间网络 $f_\theta$ 是具有 $5$ 层隐藏层的 MLP，每层宽度为 $128$；权重网络 $w_\theta$ 为 $3$ 层隐藏层的 MLP，每层宽度为 $400$，两个网络的激活函数都使用 ELU。

值得注意的是，多维求积方法中包括的子集选取和权重确定两个步骤，图 3-8 的网络结构中仅包含一个权重预测网络，子集的选取在训练之前通过最远点采样确定，在整个训练过程保持不变，权重网络在训练过程中输出不同子空间状态 $\boldsymbol{z}$ 的权重。

为了验证最远点采样方式选取子集的有效性，利用一个已经训练好的子空间网络替换图 3-8 中的子空间网络 $f_\theta$，并且冻结参数，只训练权重网络。图 3-9 展示了随机采样和最远点采样选取不同采样点数的梯度近似相对误差，从图中可以看出，FPS 采样在不同的采样点数的情况下都明显优于随机采样，因此采用 FPS 采样确定子集。

### 3.4.2 损失函数及训练策略

对于子空间网络 $f_\theta$，其损失函数如前所述，总体损失函数是势能损失、等距损失和 $Lipschitz$ 损失之和：

$$
\mathcal{L} = \mathcal{L}_p + \mu \mathcal{L}_{\text{iso}} + \lambda \mathcal{L}_{\text{LS}}
$$

对于权重网络，其作用是根据选出的子集输出当前子空间状态 $\boldsymbol{z}$ 对应的四面体的非负权重。因此，权重网络使用势能梯度的 $L_2$ 误差作为损失函数：

$$
\mathcal{L}_{\text{fit}} = \frac{1}{N} \sum_{i=1}^N \| \tilde{\boldsymbol{g}}^i - \boldsymbol{g}^i \|^2
$$

其中 $N$ 为 batch 的大小，$\tilde{\boldsymbol{g}}^i$ 是 batch 第 $i$ 帧的近似梯度，$\boldsymbol{g}^i$ 是 batch 中第 $i$ 帧的精确梯度。

值得注意的是，在进行网络训练时子空间拟合网络和权重预测网络并不是同时训练的，而是采用交替训练的方式。具体地，采用两个优化器，一个优化器负责优化子空间网络的参数，另一个负责优化权重网络的参数。首先训练权重网络，每次优化只更新权重网络的参数，经过 $100$ 次迭代之后，停止训练权重网络，开始训练子空间网络，每次优化只更新子空间网络的参数，训练 $100$ 次迭代之后停止训练转而训练权重网络。整个训练过程兩個網絡交替訓練，直到收斂。

## 3.5 实验结果及分析

为了验证本章提出的神经子空间仿真加速方法的有效性，本章分别在不同的模型上训练子空间网络，并与原始方法进行仿真速度对比和质量对比，并对实验结果进行分析。同时，探讨在本方法中一些关键参数对仿真速度和质量的影响，并对不同阶数的 $Lipschitz$ 损失训练的神经子空间进行仿真速度对比实验。

本文使用 JAX 实现仿真框架和神经网络相关算法，使用 Adam 优化器进行网络参数优化，学习率初始化为 $10^{-3}$，每 $8000$ 次迭代衰减一次，衰减系数为 $0.8$，batch 大小设置为 $256$，训练 $12000$ 个 epoch，$Lipschitz$ 损失函数的超参数 $\lambda$ 设置为 $0.001$。以上参数是基于监督学习的子空间方法的网络训练参数。对于无监督学子空间方法的网络训练参数将在相关小节进行说明。

在仿真实验中，设置时间步长 $h$ 为 $0.05s$，涉及到的梯度和 Hessian 计算均使用 JAX 的自动微分，使用进行线搜索的 L-BFGS 作为求解器。所有训练及仿真均在一台具有 Intel Xeon Silver 4216 CPU 和 NVIDIA GeForce RTX 3090 显卡的服务器上进行。

表 3-1 展示了实验中使用的网格模型的参数，其中 $n$ 是网格的自由度数，等于其顶点数乘三，$E$ 是网格的四面体数，$r$ 是选择的子空间维度，$S$ 是在多维求积近似的子集大小。

| 模型     | $n$   | $E$   | $r$ | $S$ | 数据集大小 (帧) |
|----------|-------|-------|-----|-----|------------------|
| Dinosaur | 23K   | 29K   | 40  | 500 | 2900             |
| Bunny    | 44K   | 53K   | 40  | 300 | 3100             |
| Elephant | 38K   | 62K   | 65  | 400 | 2900             |
| Bar      | 4.7K  | 3.1K  | 10  | 300 | 3000             |
| Bar-2D*  | 1.6K  | 1K    | 8   | —   | —                |
| Cloth*   | 7K    | 4.5K  | 8   | —   | —                |

*表示无监督学习神经子空间方法上使用的模型。

### 3.5.1 多维求积近似在网络训练中的有效性评估

本章提出的 $Lipschitz$ 优化方案在训练时具有计算和显存开销大的问题，在本章中提出使用多维求积方法近似计算二阶导数以解决上述问题。

首先进行实验检验使用多维求积方法在训练时的显存消耗和训练时间，以评估多维求积方法在降低显存消耗和加速训练的可行性。实验中，在 Bunny 模型上训练引入 $Lipschitz$ 优化目标的子空间网络，记录不同子空间维度下训练时的显存和时间消耗。从图 3-10 可以看出，多维求积近似极大降低了训练时的显存消耗和训练时间，即使是在较大的子空间维度下，显存开销远远低于阈值，并且训练时间也可以接受。而不使用多维求积近似的训练方法在子空间维度小于 $10$ 时就已经超过显存阈值了，训练时间难以接受。这说明在训练时用多维求积近似计算势能关于 $\boldsymbol{z}$ 的二阶导数，在降低训练时的显存消耗和计算开销是非常有效的。

为了探究多维求积方法采样点数对 $Lipschitz$ 损失函数精度的影响，以及选取合适大小的子集，分别在子集大小为 $100, 200, 300, 400, 500, 600, 700$ 评估 Hessian 和 $Lipschitz$ 损失函数的相对误差，相对误差评估在测试集上进行，通过本章提出的仿真框架生成 $700$ 帧测试数据。如图 3-11 所示，随着采样点数的增加，Hessian 的相对误差及 $Lipschitz$ 损失的相对误差逐步下降。但是更多的采样点导致多维求积的训练时间迅速增加，这是因为在求解权重时需要求解一个非负最小二乘问题，更大的子集导致非负最小二乘问题的规模扩大，增加求解时间。在综合考虑多维求解的近似精度和生成时间之后，通常选择子集大小在 $500$ 以内，这既能在较短的时间内完成训练，同时在相对误差也和更大子集的情况下没有太大差距。

### 3.5.2 仿真速度对比

本章提出通过 $Lipschitz$ 优化在神经子空间构造过程中降低势能函数在神经子空间中的复杂性，通过构造一个高效求解的子空间從而在子空间仿真中更快速地求解。为了验证本章提出的子空间仿真加速方法的有效性，在不同的模型上进行仿真速度对比使用。本章提出的方法首次从优化问题复杂性的角度来实现子空间仿真加速，也是首次以降低优化问题复杂性作为一个优化目标构造神经子空间，因此，本章提出的神经子空间仿真加速方法与現在廣泛使用的多維求積方法是正交的，可以同時使用加速神經子空間仿真。由於目前還沒有相同性質的研究工作，因此本章對比的方法是 Fulton 等人提出的神經子空間方法，也是本章構造神經子空間的方法，只是未引入 $Lipschitz$ 優化，在後文中也稱原始方法。

為了驗證本章方法在加速神經子空間仿真上的有效性，在 Dinosaur 和 Bunny 模型上進行了仿真速度對比。首先分別訓練引入 $Lipschitz$ 優化的子空間網絡和未引入 $Lipschitz$ 優化的子空間網絡，然後生成隨機交互序列，用來進行隨機交互仿真，記錄下仿真過程中每個時間步的時間消耗，總共進行 $30$ 個隨機交互，仿真幀數為 $700$。

圖 3-12 展示了 Dinosaur 模型和 Bunny 模型的部分仿真狀態幀的可視化結果和時間步時間消耗的分佈圖，分佈圖中的曲線是概率密度曲線。从图 3-12 的两个仿真时间步耗时分布图中可以看出，通过本章提出的 $Lipschitz$ 优化构造的神经子空间在仿真时仿真时间步的耗时总体上都大幅度降低，与原始方法相比，时间步耗时大多都分布在较低区间。从可视化结果来看，本章方法与 Fulton 等人的方法保持了仿真状态的一致，只在小部分区域有细微差别，比如 Dinosaur 的尾巴。这说明在進行增加了 $Lipschitz$ 優化目標之後，本章方法構造的子空間在仿真質量上並沒有明顯變差，保證了一定的仿真精度。

表 3-2 對比了兩種方法在不同模型上仿真的量化結果。表中仿真的時間步耗時是所有時間步耗時的平均值，表格中括號內的數字表示本章方法相對於對比方法的加速比。可以明顯看出，在表中的四個模型中，本章提出的神經子空間加速方法構造的子空間相比於原始神經子空間在仿真上實現了兩倍以上的加速比，並且在 Dinosaur 模型上達到了 $6.88$ 倍。這裡 Dinosaur 的加速比遠大於其他模型的原因可能是 Dinosaur 模型在形狀上相比於其他模型具有更多的高頻細節，並且 Dinosaur 在仿真中運動狀態比較複雜，跨越的空間範圍較大，因此它的優化空間較大。表 3-2 中還與全空間方法的仿真速度進行了對比，可以看到，本章方法相比於全空間方法都實現了一個數量級以上的加速，在複雜的模型中，加速比更大。這說明本章方法在加速神經子空間仿真中的有效性。

表 3-2 不同模型上本章方法與 Fulton 等人的方法的仿真速度對比

| 模型     | $n$   | $r$ | 本章方法 (ms) | Fulton (ms) | 全空间 (ms) |
|----------|-------|-----|----------------|---------------|---------------|
| Dinosaur | 23K   | 40  | 9.1            | 62.6 (6.88×)  | 1182.4 (129.93×) |
| Bunny    | 44K   | 40  | 4.5            | 21.0 (4.67×)  | 687.2 (154.43×) |
| Elephant | 38K   | 65  | 14.1           | 38.9 (2.76×)  | 464.2 (32.85×) |
| Bar      | 4.7K  | 10  | 4.4            | 12.3 (2.80×)  | 115.6 (26.22×) |

### 3.5.3 仿真质量对比

为了探究在进行二阶 $Lipschitz$ 优化后子空间仿真的质量，将本章方法和 Fulton 等人的方法进行仿真质量对比。首先，将本章方法和 Fulton 等人的方法与全空间仿真进行对比，全空间仿真结果作为 Ground Truth。两个方法分别进行相同的仿真，计算与全空间仿真结果的最大顶点误差 $|d|_\infty$，这里的最大顶点误差是一帧中网格所有顶点与 Ground Truth 对应帧的对应顶点之间的距离的最大值，使用的是绝对误差，本文使用的所有网格的包围盒都是 $1$m。图 3-13 展示了两种方法在 Bar 模型上进行仿真的最大顶点误差，图中的仿真是一个长方体的弹性棒先扭转然後壓縮的過程。

图 3-13(a) 展示了本章方法与 Fulton 等人的方法在仿真中两个状态的可视化结果，并且与 Ground Truth 进行重叠，红色部分为 Ground Truth。可以看到，两种方法的仿真状态几乎一致，并且最大顶点误差也很接近。图 3-13(b) 是本章方法与 Fulton 等人的方法在仿真中的最大顶点误差分布，两种方法的最大顶点误差都聚集在较低的区域，并且本章方法的误差更集中。这从数值上说明了本章方法在提高子空间仿真速度的同時保證了一定的仿真精度。

为了探究两种方法构造的子空间的表示能力，本章将评估两者的投影误差。投影误差是子空间映射 $f_\theta$ 把子空间状态映射到全空间产生的误差：$\| f_\theta(z_i) - \boldsymbol{q}_i \|$。为了计算投影误差，需要找到全空间状态对应的子空间状态。可以使用子空间构造时训练的编码模块 $g_\theta$ 将全空间状态投影到子空间中获得一个子空间状态 $z_{\text{init}}$，由于编码的过程也会产生误差，所以这个子空间状态也不一定对应当前全空间状态。为了找到某一全空间状态 $\boldsymbol{q}_i$ 对应的子空间状态 $z_i$，构造一个优化问题：

$$
\min_z \| f_\theta(z) - \boldsymbol{q}_i \|^2
$$

将 $z_{\text{init}}$ 作为初始解，并使用 L-BFGS 算法求解。

从全空间仿真中获取一部分轨迹数据作为 Ground Truth，将这部分数据通过上述方法计算其对应的子空间状态，并计算投影误差。如图 3-14 所示，在 Elephant 模型上进行投影误差對比實驗，其中投影誤差使用最大頂點誤差評估，圖左側是某一幀中本章方法與 Fulton 等人方法的投影狀態與 Ground Truth 的重疊，紅色部分為 Ground Truth，兩者在當前幀的投影誤差從數值上看可以認為一致。從右側的投影誤差分佈圖中可以看出，兩者的投影誤差概率密度曲線幾乎重疊，這說明兩個子空間的表示能力是相似的。

通過以上兩個實驗可知，本章方法在仿真質量上與原始方法幾乎一致。這說明利用神經網絡的學習能力，可以在保持低二階 $Lipschitz$ 常數的條件下找到一個與原始方法具有相似的像（Image）的子空間。

### 3.5.4 关键参数和损失函数探讨

本节将探讨子空间维度大小和求解器收敛阈值对仿真的影响，同时对不同阶数的 $Lipschitz$ 损失函数训练的模型进行对比实验。

#### 子空间维度对仿真效果的影响

为了探究不同子空间维度的选择对仿真质量和仿真速度的影响，分别选择子空间维度大小为 $15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65$ 训练子空间网络，计算不同子空间维度下的投影误差和仿真时间步耗时，通过一个可视化的例子进行說明，實驗結果如圖 3-15 所示。

圖 3-15(a) 展示了同一交互下，Elephant 模型在到達穩態之後不同維度的子空間仿真結果。可以明顯看出，在較小的子空間維度下，模型表現出較強的數值剛度，$15$ 維的子空間在相同的交互下鼻子拉起的幅度顯然不如 $45$ 維和 $65$ 維的子空間，隨著子空間維度的增大，數值剛度越小，模型表現得越軟。

圖 3-15(b) 展示了不同子空間維度下投影誤差與仿真時間步耗時對比。隨著子空間維度的增大，本章方法和 Fulton 等人方法的投影誤差緩慢下降，並且兩者差距不大；兩者的仿真時間步耗時隨著子空間維度增加而上升，但是上升的速度差別較大，Fulton 等人的方法在子空間維度增加時時間步耗時上升迅速，而本章方法的時間步耗時上升緩慢，這說明本章提出的 $Lipschitz$ 優化的有效性。因此，可以使用本章方法構造維度較大的子空間，在更快的仿真速度下實現更高精度的仿真。

#### 求解器收敛阈值的影响

本文的仿真框架使用 L-BFGS 求解器求解時間積分，其中收斂條件至關重要。本文使用梯度的無窮范數 $\| \boldsymbol{g} \|_\infty$ 作为收敛条件，在所有的仿真实验中，收敛阈值 $\varepsilon = 10^{-5}$，这也是 JAX 的 L-BFGS 算法的默认阈值。为了探究收敛阈值对本章方法和原始方法的子空间仿真的影响，在收敛阈值为 $10^{-4}, 10^{-5}, 10^{-6}$ 下分别对本章方法和 Fulton 等人方法训练的神经子空间进行仿真，所用网格模型为 Dinosaur，设置四个退出条件，记录下退出时触发的退出条件占比、退出时的梯度范数 $\| \boldsymbol{g} \|_\infty$ 及平均仿真时间步耗时 $\bar{t}$。

实验结果如表 3-3 所示。可以看到在不同的收敛阈值下本章方法大部分情况下都能收敛，而 Fulton 等人的方法在较小的收敛阈值下难以收敛，因此，本章方法在退出求解时 $\| \boldsymbol{g} \|_\infty$ 比 Fulton 方法的小。从平均仿真时间步耗时看，三个收敛阈值下的加速比分別為 $8.45, 6.88, 2.25$，在較小收斂閾值的情況下，本章方法相比於 Fulton 等人方法的加速比更大，在較大的收斂閾值下加速比減小。這是因為 Fulton 等人方法已經由於難以收斂提前退出了，而本章方法還在繼續迭代接近最優解，從退出時的 $\| \boldsymbol{g} \|_\infty$ 可以看出這一點。這個實驗說明本章提出的 $Lipschitz$ 優化降低了時間積分的複雜性。

| 退出条件 | $\varepsilon$ | 方法   | $\| \boldsymbol{g}_{\text{exit}} \|_\infty$ | $\bar{t}$ (ms) |
|----------|---------------|--------|---------------------------------------------|----------------|
| I        | $10^{-4}$     | 本章   | $7.2 \times 10^{-5}$                        | $3.7$          |
| I        | $10^{-4}$     | Fulton | $8.8 \times 10^{-5}$                        | $31.3$         |
| I        | $10^{-5}$     | 本章   | $8.3 \times 10^{-6}$                        | $9.1$          |
| II       | $10^{-5}$     | Fulton | $5.1 \times 10^{-5}$                        | $62.6$         |
| I        | $10^{-6}$     | 本章   | $1.8 \times 10^{-6}$                        | $28.2$         |
| II       | $10^{-6}$     | Fulton | $5.0 \times 10^{-5}$                        | $63.4$         |

退出条件说明：  
I：收敛，无穷范数小于阈值 $\varepsilon$  
II：到达鞍点  
III：达到最大线搜索次数  
IV：达到最大 L-BFGS 迭代次数

#### 不同阶数 Lipschitz 损失函数的对比

如前所述，可以通过改变 $Lipschitz$ 损失函数的导数阶数获得不同阶的 $Lipschitz$ 损失函数：

$$
\mathcal{L}_{\text{LS}, o} = \mathbb{E}_{\boldsymbol{z}_i, \boldsymbol{z}_j \sim \Pi_\theta(\boldsymbol{z})} \left[ \frac{\left\| \frac{\partial^o P}{\partial \boldsymbol{z}^o}(\boldsymbol{z}_i) - \frac{\partial^o P}{\partial \boldsymbol{z}^o}(\boldsymbol{z}_j) \right\|^2}{\| \boldsymbol{z}_i - \boldsymbol{z}_j \|^2} \right]
$$

其中 $o$ 为导数的阶数。

本章将对不同阶的 $Lipschitz$ 损失函数的效果进行探索，分别使用不同阶数的 $Lipschitz$ 损失函数训练神经子空间网络。

表 3-4 展示了不同阶 $Lipschitz$ 损失函数训练的子空间网络在仿真中的平均时间步开销和各阶 $Lipschitz$ 常数的大小，这里取 $o \leq 2$。可以看出二阶的 $Lipschitz$ 损失在子空间仿真加速求解上效果最好，這與之前的分析一致，並且優化二階 $Lipschitz$ 常數會降低更低階的 $Lipschitz$ 常數。值得注意的是，這裡的 $0$ 階 $Lipschitz$ 優化導致仿真速度下降了，可能的原因是 $0$ 階 $Lipschitz$ 損失函數讓網絡傾向於學習一個常勢能函數，但是網絡還具有重建損失，所以勢能函數不可能是常函數，因此 $0$ 階 $Lipschitz$ 優化使網絡學習到一個非常崎嶇的映射。從表中也可以看出，$0$ 階 $Lipschitz$ 損失使勢能函數的一階、二階 $Lipschitz$ 常數都變大了。

| $Lipschitz$ Loss | 时间步耗时 (ms) | $\mathrm{Lip}[P]$ | $\mathrm{Lip}[\nabla_{\boldsymbol{z}} P]$ | $\mathrm{Lip}[\nabla_{\boldsymbol{z}}^2 P]$ |
|------------------|------------------|-------------------|-------------------------------------------|---------------------------------------------|
| 无               | 21.0             | 1.84              | 9.59                                      | 42.58                                       |
| $\mathcal{L}_{\text{LS},0}$ | 25.3             | 1.18              | 18.37                                     | 108.41                                      |
| $\mathcal{L}_{\text{LS},1}$ | 13.0             | 0.49              | 1.79                                      | 6.40                                        |
| $\mathcal{L}_{\text{LS},2}$ | 4.5              | 0.32              | 0.17                                      | 0.34                                        |

### 3.5.5 无监督学习方法上的表现

本章將提出的 $Lipschitz$ 優化方法應用到基於無監督學習的神經子空間方法上，下面將進行實驗驗證本章提出的神經子空間加速方法在無監督學習子空間方法上的有效性。在無監督方法上的實驗，有兩種訓練方案。對於分辨率較低的模型，採用精確計算 $Lipschitz$ 損失函數的形式進行訓練，在原始方法上直接加入 $Lipschitz$ 損失；而對於分辨率較高的模型，採用 3.4 節的方法進行訓練，訓練子空間網絡的同時訓練一個額外的權重預測網絡。兩種方式的訓練參數保持一致，batch 大小設為 $32$，網絡總共訓練 $10^6$ 次迭代，學習率設為 $10^{-4}$，$Lipschitz$ 損失的超參數 $\lambda$ 设为 $0.001$。

在仿真質量對比實驗中，使用與第三章相同的 Dinosaur 模型，分別用本章方法和原始方法進行隨機交互仿真，記錄每個時間步的用時，總共進行 $30$ 個隨機交互，仿真步數為 $700$。如圖 3-16 所示，通過引入 $Lipschitz$ 優化後，Dinosaur 模型的仿真時間步用時明顯降低，且與原始方法相比，時間步用時大多分佈在較低區間。這說明本章提出的加速方法在無監督學習的子空間方法上也能有效加速仿真過程。

表 3-5 展示了無監督方法上三個模型的平均仿真時間步耗時，其中 Bar-2D 和 Cloth 在訓練時是精確計算 $Lipschitz$ 損失的，而 Dinosaur 通过本章 3.3 節提出的方法進行訓練，選擇的 Cubature 子集大小 $S$ 為 $300$。從表中可以看出，本章的加速方法在三個模型上都實現了加速。但是加速比來看，本章方法在無監督學習方法上的加速效果遠遠不如在基於監督學習的神經子空間上的表現，這主要是因為當前提出的無監督神經子空間方法只能擬合較小的子空間，增大子空間的維度之後網絡訓練得到的子空間映射的質量並沒有提高，並且使訓練變得不穩定，這也是當前基於無監督學習的神經子空間方法的缺陷 [17]。

| 模型     | $n$   | $r$ | $S$ | 本章方法 (ms) | Sharp (ms) | 加速比 |
|----------|-------|-----|-----|----------------|-------------|--------|
| Bar-2D   | 1.6K  | 8   | —   | 2.1            | 3.9         | 1.86×  |
| Cloth    | 7K    | 8   | —   | 16.3           | 23.1        | 1.42×  |
| Dinosaur | 23K   | 20  | 300 | 6.2            | 10.3        | 1.66×  |

### 3.5.6 实验结果汇总表

表 3-6 展示了本章方法与原始方法 (Vanilla) 在六个模型上训练时间、二阶 $Lipschitz$ 常数 $\mathrm{Lip}[\nabla_{\boldsymbol{z}}^2 P]$ 和仿真时间步耗时的对比结果。通过本章提出的 $Lipschitz$ 优化，势能函数在神经子空间中的二阶 $Lipschitz$ 常数大幅度降低，從而加速了神經子空間仿真。由於 $Lipschitz$ 損失的引入，子空間網絡的訓練時間大幅度上升，雖然本章中使用了多維求積近似加速計算勢能函數的二階導數，但網絡訓練的時間還是極大增加了。通過增加離線的預計算時間換取更快的在線仿真效率對於仿真實時性要求較高的應用是非常有價值的，這也是當前子空間仿真方法所追求的。

| 模型     | $\mathrm{Lip}[\nabla_{\boldsymbol{z}}^2 P]$ (本章) | $\mathrm{Lip}[\nabla_{\boldsymbol{z}}^2 P]$ (Vanilla) | 训练时间 (h) (本章) | 训练时间 (h) (Vanilla) | 时间步耗时 (ms) (本章) | 时间步耗时 (ms) (Vanilla) |
|----------|-----------------------------------------------------|--------------------------------------------------------|----------------------|------------------------|-------------------------|---------------------------|
| Dinosaur | 215.8                                               | 18.3                                                   | 4.2                  | 0.3                    | 9.1                     | 62.6                      |
| Bunny    | 42.6                                                | 10.0                                                   | 1.7                  | 0.3                    | 4.5                     | 21.0                      |
| Elephant | 117.5                                               | 22.3                                                   | 4.7                  | 1.3                    | 14.1                    | 38.9                      |
| Bar      | 299.7                                               | 2.6                                                    | 0.6                  | 0.3                    | 4.4                     | 12.3                      |
| Bar-2D*  | 30.9                                                | 1.0                                                    | 0.4                  | 0.4                    | 2.1                     | 3.9                       |
| Cloth*   | 9.0                                                 | 9.0                                                    | 0.9                  | 3.8                    | 16.3                    | 23.1                      |
| Dinosaur*| 12.8                                                | 10.4                                                   | 6.1                  | 0.8                    | 6.2                     | 10.3                      |

*表示无监督学习神经子空间方法。

## 3.6 本章小结

本章构建了一个可以进行随机交互仿真的仿真框架，并提出了随机交互的生成方法，通过这个仿真框架可以自动生成高质量的仿真轨迹数据集，進行神經子空間網絡的訓練。在完成數據集的構造之後，本章提出一個基於 $Lipschitz$ 優化的神經子空間仿真加速方法，在訓練神經子空間網絡的過程中引入 $Lipschitz$ 損失函數，降低勢能函數在子空間中的複雜性，從而在仿真時快速求解。為了降低 $Lipschitz$ 損失函數的計算量降低顯存的消耗，通過多維求積近似計算 $Lipschitz$ 損失函數中勢能函數的二階導數，大大加速了訓練和降低了顯存消耗。本章將提出的加速方法和與未進行 $Lipschitz$ 優化的原始方法進行仿真速度對比和質量對比，並且探討了本方法中一些關鍵參數對仿真的影響。實驗結果表明，本章提出的神經子空間加速方法可以在保證仿真精度的條件下極大加速神經子空間仿真。同時，將本章提出的神經子空間加速方法應用到基於無監督學習的神經子空間方法上，並通過實驗說明本文提出的加速方法在無監督神經子空間方法上的有效性。

# 第四章 基于神经网络的可变权重多维求积方法

## 4.1 引言

子空间方法通过降低时间积分中优化问题的维度极大加速了可变形体仿真的效率，但是在优化求解时需要计算势能的一阶导数或二阶导数，这些导数的计算开销仍与全空间的维度相关，多维求积（Cubature）加速了子空间梯度的计算，极大加速了子空间仿真。图 4-1 展示了使用多维求积方法的子空间仿真时间步耗时分布，左侧的 $E$ 表示均值，可以看到，多维求积方法实现了接近 $4$ 倍的加速比。因此，多维求积方法在子空间仿真中是不可缺少的一种加速方法。

多维求积的关键是子集的选取和权重的确定，当前的子空间仿真使用的多维求积方法都使用的是固定权重方案，在确定子集之后权重在整个仿真的过程中都不会改变，这限制了多维求积的近似精度，需要在较大的子集下才能达到较高的近似精度。最近，Shen 等人 [13] 提出了可变权重的多维求积方法。该方法通过子集选择网络输出每个四面体的重要性，每次选择重要性较大的四面体，在选出子集之后，通过权重预测网络拟合权重。他们的子集选择网络每次可以选择多个四面体，提高了子集的构造速度，然而这种子集选取方式並不能選出較好的子集，因此他們的方法在近似精度上與固定權重的方案相當。

本章提出一种基于神经网络的可变权重多维求积方法，在使用固定权重的方法确定子集和权重之后，通过训练一个权重网络修正当前子空间状态的权重，提高多维求积方法近似精度。本章的主要研究内容如下：

(1) 提出一个快速子集选取方法，该方法在当前贪心策略的子集选择方法基础上，通过权重局部更新和全局更新降低了选取子集时求解的非负最小二乘法的规模，在保证近似精度的同时，大幅度加快了子集选取的速度；

(2) 提出一个可变权重的多维求积方法，在确定子集和权重之后，训练一个权重修正网络，在仿真时根据当前的子空间坐标输出权重修正向量，实现仿真运行时的可变权重方案，提高多维求积方法的近似精度；

(3) 对提出的快速子集选取方法和可变权重多维求积方法进行实验及分析。为了评估方法的有效性，将提出的方法与现有的最广泛使用的方法 [21][22] 进行对比实验，证明该方法在训练时间和近似精度上的优势。同时，将本章提出的可变权重多维求积方法应用在第三章提出的基于 $Lipschitz$ 优化的神经子空间加速方法中，证明该方法在加速子空间网络训练上的有效性。

## 4.2 可变权重的多维求积方法

本文在第三章提到，对于一个优化问题，求解开销主要由三部分决定：计算目标函数及其导数的开销 $C_{\text{eval}}$，计算下降方向和步长的开销 $C_{\text{dir}}$（求解线性系统计算下降方向和线搜索确定步长），以及迭代次数 $n_{\text{iter}}$，总的求解开销大致为：

$$
n_{\text{iter}} (C_{\text{eval}} + C_{\text{dir}})
$$

通过子空间方法降维，在计算下降方向时求解的线性系统的维度大幅度降低，因此在子空间仿真中，$C_{\text{dir}}$ 的开销大幅度下降；通过本文第三章提出的基于 $Lipschitz$ 优化的神经子空间仿真加速方法，迭代次数 $n_{\text{iter}}$ 也已经降低；通过多维求积方法近似子空间势能梯度的计算，大幅度降低计算目标函数导数的开销 $C_{\text{eval}}$。

多维求积方法已经在子空间仿真中广泛使用，极大加速了子空间仿真。然而，当前的多维求积方法在训练时间和近似精度上还有不足，需要较大的子集才能达到较高的近似精度，这將導致多維求積方法的訓練時間過長。本章提出一個可變權重的多維求積方法，在相同大小的子集下實現比固定權重方法更高的近似精度，並且通過改進基於貪心策略的多維求積方法，極大加速了本章提出方法的預計算時間。如圖 4-2 所示，本章提出的可變權重的多維求積方法基於當前的固定權重方法，首先使用當前固定權重的多維求積方法從訓練數據中獲取積分子集和初始權重，在此基礎上訓練一個權重修正網絡針對不同的子空間坐標輸出當前狀態的權重修正向量，權重修正向量和初始權重之和就是當前狀態的權重。

### 4.2.1 基于贪心策略的快速子集选取

本章方法首先使用固定权重的多维求积方法确定子集和初始权重，一个高质量的多维求积方法是非常重要的。当前在各种子空间通用的方法只有 An 等人提出的 Greedy Cubature 算法 [21] 和 Von Tycowicz 提出的 NN-HTP 算法 [22]，考虑到 Greedy Cubature 在各种子空间下通常近似精度更高，本章将使用 An 等人提出的方法作为本章方法的子集选取方案。

首先考虑权重求解的问题，在已经选出子集的情况下，最小化子空间势能梯度的相对误差求解非负权重：

$$
A \boldsymbol{w} = \boldsymbol{b}
$$

其中 $A \in \mathbb{R}^{rT \times m}$，$A$ 是由已选择的四面体的势能梯度组成的矩阵，$m$ 是已经选择的四面体数，$T$ 为训练数据的帧数，$r$ 是子空间维度，$A$ 的每一列是所有数据帧中四面体 $i$ 的势能梯度组成的列向量，$\boldsymbol{b}$ 是由所有数据帧的精确梯度组成的列向量，$\boldsymbol{w}$ 是非负权重向量。这里优化的是相对误差，线性方程组的每一行除以对应帧的精确梯度的模长。由于训练数据帧通常较多，$m$ 通常小于 $1000$，则 $rT > m$，上式是一个超定的线性方程组，只能求出近似解，因此使用非负最小二乘法 (NNLS) 求解上式。

对于子集的选取，An 等人采用贪心策略动态地构造出完整的子集，通过选取当前状态下可以最小化残差的四面体、更新子集、求解非负最小二乘法、更新残差这个循环来动态构造积分子集。整个算法流程如算法 1 所示：

（算法 1 伪代码略，见原文）

An 等人提出的 Greedy Cubature 算法通过贪心策略选择子集，每次选择最小化当前残差的四面体加入子集，这种选取方法在实践中证明是有效的。然而他们的方法每次只选择一个四面体就要求解一次非负最小二乘法，随着子集的增大，上式的规模越来越大，非负最小二乘法的求解时间极大增加，使这种子集选取方法效率低下。本章通过權重局部更新和全局更新的方式降低每次求解的非負最小二乘法的規模，提出了 Modified Greedy Cubature (MGC) 算法，加速基於贪心策略的子集選取。

（算法 2 伪代码略，见原文）

如算法 2 所示，整个算法的框架和 An 等人的 Greedy Cubature 一致，都是通过选取四面体更新子集、求解非负最小二乘法、更新残差这个循环来动态构造积分子集。与 An 等人的算法不同的是，本章算法將整個子集的構造問題劃分為多個更小的子集構造問題，在小子集構造問題中只更新這個子問題中選擇的四面體的權重，其他權重固定；在求解完一個小子集的構造問題之後，通過求解一次完整的非負最小二乘法進行權重的全局更新。

为了验证本章提出的快速子集选择方法的有效性，通过一个简单的实验进行初步验证，在 Dinosaur 模型上用本章提出的方法和 An 等人的方法分别训练 Cubature，$n$ 设为 $300$，$\text{sub}_n$ 设为 $20$，所用的子空间为 PCA 子空间，子空间维度为 $131$。图 4-3 展示了本章提出的方法和 An 等人的方法在训练 Cubature 时的相对近似误差曲线。可以看出，两种方法的曲线接近，图中本章方法的曲线是分段的，兩個分段之間表示全局更新，因為通過一次全局更新誤差會有一次下降。

表 4-1 展示了两种方法在子集大小为 $300$ 时的相对近似误差和训练时间，可以看出本章提出的快速子集选择方法极大加快了子集的选取速度，并且相对近似误差和 An 等人的方法也很接近。这初步说明了本章的快速子集选择方法的有效性。更进一步的实验将在实验结果及分析部分介绍。

| 方法     | 相对近似误差 | 训练时间 (h) |
|----------|--------------|--------------|
| An 等人  | 0.194        | 2.0          |
| 本章方法 | 0.201        | 0.2          |

表 4-3 展示了 MGC 与 GC、NN-HTP 在神经子空间下的表现，神经子空间比 PCA 子空间具有更小的维度，因此相比於 PCA 子空間能夠在更小的積分子集下達到相當的近似精度。從表中可以看出，GC 和本章方法的相對近似誤差明顯小於 NN-HTP，GC 方法的相對近似誤差是最低的。並且與 PCA 子空間上的結果不同的是，GC 的訓練時間並沒有遠高於 NN-HTP 和本章方法，這是因為在子空間維度和子集較小的情況下非負最小二乘法的規模較小，因此求解較快。而 NN-HTP 雖然迭代次數較少，但是在迭代過程中除了要求解非負最小二乘法還要進行梯度下降，所以訓練時間並沒有在大子空間下那麼有優勢。本章方法與在 PCA 子空間上的表現一致，在相對近似誤差接近 GC 的同時，訓練時間相對於 GC 也大幅度降低。

通过以上两个实验可知，本章提出的快速子集选取方法在与当前最通用的方法 GC 具有相当近似精度的条件下大幅度降低了训练时间，说明了本章提出的快速子集选取方法的有效性。同时，NN-HTP 算法在子空间维度较小的情况下近似精度与 GC 已经有明显差距，并且也没有很大的训练时间优势。因此，本章的初始子集和权重获取方法选择 GC。

### 4.3.2 可变权重的多维积分方法的有效性

本章通过训练一个权重修正网络输出不同子空间坐标的权重修正向量，提高在固定子集下的势能梯度近似精度。为了验证本章提出的可变权重的多维积分方法的有效性，在固定子集大小下训练 Cubature，比较本章方法与 GC 在测试集上的势能梯度相对近似误差和训练时间。其中，测试集数据包括 $700$ 帧，通过的随机交互序列从仿真框架中获取。

表 4-4 展示了本章方法与 GC 在不同模型上的相对近似误差和训练时间，其中本章方法的训练时间中括號內的部分是權重修正網絡的訓練時間，由於網絡的規模很小，所以可以在較短的時間內完成訓練。從表 4-4 可以看出，在通過神經網絡修正權重之後，勢能梯度的相對近似誤差有明顯下降，通過本章提出的快速子集選取方法 (MGC)，整個可變權重多維求積方法的訓練時間相對於 GC 有巨大優勢。

| 模型     | $r$ | $S$ | GC 误差 | 本章方法误差 | GC 时间 (min) | 本章方法时间 (min) |
|----------|-----|-----|----------|---------------|------------------|----------------------|
| Dinosaur | 40  | 500 | 0.068    | 0.041         | 157              | 11 (2)               |
| Bunny    | 40  | 500 | 0.050    | 0.037         | 205              | 23 (2)               |
| Elephant | 65  | 500 | 0.044    | 0.027         | 265              | 19 (3)               |

为了说明引入权重修正网络的必要性，增大子集大小使 GC 和 MGC 训练到与表 4-4 中本章方法一致的相对近似误差，结果如表 4-5 所示。可以看出，在子集较大时相对近似误差的下降速率变慢，因此 GC 和 MGC 都需要增加大量四面体才能到达与本章方法相同的近似误差，此时训练时间也大幅度增加。因此在已经达到较高的近似精度下引入权重修正网络可以明显降低误差，而带来的额外成本卻很低。權重修正網絡的引入會帶來兩個額外開銷，一是訓練時間開銷，由於網絡的規模很小，因此訓練開銷很小；二是仿真時權重修正網絡的前向過程開銷，在仿真實驗中，發現網絡的前傳開銷為 $0.1$ ms，因此引入的前傳開銷可以忽略。

| 方法 | $r$ | $S$ | 相对近似误差 | 训练时间 (min) |
|------|-----|-----|----------------|------------------|
| GC   | 40  | 683 | 0.041          | 533              |
| MGC  | 40  | 680 | 0.041          | 22               |

为了探究多维求积方法的近似精度对仿真的影响，将使用表 4-4 训练的 Cubature 进行仿真实验。将不使用 Cubature 方法的子空间仿真作为 Ground Truth (GT)，將 GC 和本章方法進行子空間仿真的結果與 GT 進行比較。仿真場景是 Bunny 的耳朵受到一個交互，這個交互持續 $50$ 幀，交互結束後再進行 $50$ 幀仿真。圖 4-5(b) 是仿真過程的每個時間步的最大頂點誤差，其計算方式是模型中所有頂點與 GT 對應頂點的距離的最大值，使用的是絕對誤差，模型的包圍盒是 $1$ m。圖 4-5(c) 是仿真過程的每個時間步的平均頂點誤差。從圖中可以看出，Cubature 的使用會帶來一定的仿真誤差，並且 Cubature 的相對近似誤差與其帶來的仿真誤差相關，本章方法由於近似精度更高所以引入的仿真誤差在大多數時間步中更低。從圖 4-5(b) 中取本章方法和 GC 誤差差距較大的三個時間步，其中 $t_1 = 45, t_2 = 75, t_3 = 100$，其可視化結果如圖 4-5(a) 所示，圖中綠色模型為 GT，藍色模型為使用 GC 方法的仿真，橙色模型為使用本章方法的仿真，可以明顯看出，在 Bunny 的耳朵部分使用了 Cubature 的仿真與 GT 有明顯的偏差，本章方法相對近似誤差較低因此偏差較小。

通過以上實驗可知，Cubature 的近似精度在仿真中非常重要，即使是在已經很小的相對近似誤差下，Cubature 也會對仿真結果有明顯影響，因此在較快的時間內完成高近似精度的 Cubature 的訓練具有重要意義。本章方法通過引入權重修正網絡，在相同子集大小下進一步提高了近似精度，並且通過改進 GC 和使用小規模的網絡使得本章方法的訓練時間相比於 GC 極大降低。

### 4.3.3 本章方法在神经子空间网络训练的表现

本文第三章提出的基於 $Lipschitz$ 優化的神經子空間加速方法需要計算勢能關於子空間狀態的二階導數，會帶來龐大的計算開銷，雖然通過引入多維求積近似大幅度降低了計算開銷，但其訓練時間仍然較長。通過前面實驗及分析可知，本章提出的可變權重多維求積方法在相同積分子集大小下具有比當前固定權重方法更高的近似精度，可以在相似近似精度下構造更小的子集。因此，可以通過更小積分子集的 Cubature 訓練神經子空間網絡實現網絡訓練的加速。

第三章的子空間網絡使用的多維求積方法是 NN-HTP，這裡替換為具有相似近似精度的本章方法，各個模型上的相對近似誤差如表 4-2 所示，在相同參數下訓練子空間網絡，記錄訓練時間和用訓練的子空間進行仿真的平均仿真時間步用時，其結果如表 4-6 所示。從表中可以看出，本章方法在使用更小的子集下能達到與 NN-HTP 方法相近的近似精度，所以子空間網絡的訓練時間有較大程度的降低，並且從訓練完成的子空間在仿真中的表現看，本章方法的仿真時間步耗時與用 NN-HTP 方法接近。這說明了本章方法在加速第三章基於 $Lipschitz$ 優化的神經子空間加速方法的網絡訓練上具有很好的效果。

| 模型     | NN-HTP $S$ | 本章方法 $S$ | NN-HTP 仿真耗時 (ms) | 本章方法 仿真耗時 (ms) | NN-HTP 训练时间 (h) | 本章方法 训练时间 (h) |
|----------|------------|--------------|------------------------|--------------------------|----------------------|------------------------|
| Dinosaur | 550        | 300          | 10.34                  | 10.23                    | 18.3                 | 12.1                   |
| Bunny    | 220        | 150          | 5.24                   | 4.97                     | 10.0                 | 8.9                    |
| Elephant | 400        | 200          | 14.88                  | 15.07                    | 22.3                 | 14.5                   |

### 4.3.4 消融实验

本章提出了可变权重的多维求积方法，在一个已经确定的初始权重上进行修正，還有一種更直接的方式是直接學習一個權重預測網絡輸出當前子空間狀態的權重，如同 Shen 等人所做的。為了驗證本章提出的修正方法的有效性，將與直接用網絡學習權重的方法進行比較，用 GC、權重修正和直接權重預測三種方法在 Bunny 模型上分別訓練，對比其相對近似誤差的均值、最大值、最小值和方差。

如表 4-7 所示，本章的權重修正方法在相對近似誤差的均值、最大值、最小值和方差都取得了最優，而使用權重預測網絡直接輸出權重的方法的相對近似誤差較大，甚至超過了使用固定權重的 GC，這是因為訓練數據相對於整個子空間是稀疏的，直接通過網絡預測每一點的權重很容易造成過擬合，而本章的權重修正只是對初始權重進行微調，緩解了過擬合的問題。

| 方法                     | $S$  | 均值  | 最大值 | 最小值 | 方差  |
|--------------------------|------|-------|--------|--------|-------|
| GC                       | 500  | 0.052 | 0.132  | 0.021  | 0.021 |
| MGC + 权重修正网络       | 500  | 0.037 | 0.103  | 0.010  | 0.016 |
| MGC + 权重预测网络       | 500  | 0.059 | 0.235  | 0.015  | 0.035 |

相近的子空间状态对应的形变是相近的，因此本章认为其权重也应该相近，为了使权重修正向量相对于子空间状态有一个更平滑的变化，本章的权重修正网络使用了 LipMLP，为了验证 LipMLP 的有效性，將 LipMLP 與 MLP 進行對比。如表 4-8 所示，本文使用的 LipMLP 在相對近似誤差的均值、最大值和方差上都取得了最優結果，MLP 的結果與 LipMLP 相近，但是方差更大。這說明 LipMLP 學習的平滑映射在一定程度上可以提高權重修正向量的準確性。

| 网络   | $S$  | 均值  | 最大值 | 最小值 | 方差  |
|--------|------|-------|--------|--------|-------|
| LipMLP | 500  | 0.037 | 0.103  | 0.010  | 0.016 |
| MLP    | 500  | 0.039 | 0.115  | 0.009  | 0.023 |

## 4.4 本章小结

本章针对当前多维求积方法中固定权重方案精度受限的問題提出了一個可變權重的多維求積方法。通過目前固定權重的多維求積方法從數據集中獲取積分子集和初始權重，訓練一個權重修正網絡輸出不同子空間狀態的權重修正向量，實現仿真運行時的可變權重方法。同時，本章提出通過引入權重局部更新和全局更新改進了基於貪心策略的多維求積方法，在保證相似近似精度的同時大幅度加快了其訓練時間。為了驗證本文提出的可變權重多維求積方法的有效性，與當前使用最廣泛的 Greedy Cubature 方法進行定量實驗對比，實驗證明本章方法在近似精度和訓練時間上都有明顯優勢，證明了本章方法的有效性。

## 附录A：降低 NNLS 相对误差的实践与研究建议

- 预处理与候选筛选
  - 列归一化：对 A 的列做 L2 归一化，缓解不同四面体梯度量级差异，改善 NNLS 条件数与学习稳定性。
  - 相关性候选：用 |A^T b| 排序，固定 3–5×k 列作为候选掩码，再做学习与 NNLS，显著降低噪声干扰。
- 可微选择训练技巧
  - 教师蒸馏：在候选上用 NNLS 求 w*，训练时对可微权重加 MSE 蒸馏，提升收敛速度与 RelErr。
  - 熵正则与温度退火：加入小权重熵正则并缓慢退火温度，促使分布更尖锐，评估阶段 top-k 更稳定。
  - 课程式 top-k：训练早期以较大 k 评估，随 epoch 线性或指数衰减到目标 k，避免早期过强稀疏。
- 特征改进
  - 加入列相关度与列范数：将 per-column 的 |A^T b|、||A[:,i]|| 作为强信号特征，配合已有时序/统计特征。
  - 帧内对比特征：引入 A[:,i] 与近邻列的余弦相似度，帮助模型抑制冗余列、提升多样性。
- 目标与正则
  - 多目标权衡：MSE/MAE/相对误差 + L1/L2 正则 + 熵正则 + 蒸馏损失；建议用网格搜索微调权重。
  - 支撑集一致性：对选中列的稳定性加入小的时间平滑/抖动惩罚，降低跨 batch 的选择波动。
- 数据与分割
  - 列归一化与掩码使用统一于 train/test，避免分布偏移。
  - 分层抽样：按帧梯度范数或能量分层抽样 batch，提升难样本覆盖率。
- 消融与调参建议
  - 网格：temperature∈[0.05,0.5]，entropy_weight∈[1e-4,5e-3]，distill_weight∈[5e-2,3e-1]，候选倍数∈[3,6]×k。
  - 指标：除 RelErr 外，记录选择稀疏度、重合率(与 NNLS 前 k 重叠)、稳定性(跨 epoch 交并比)。
- 扩展方向
  - 稀疏激活：尝试 sparsemax/entmax 代替 softmax（训练阶段），提升稀疏可控性。
  - 结构先验：对空间相邻四面体加图正则，鼓励局部一致，或用轻量 GNN 代替纯 Transformer。